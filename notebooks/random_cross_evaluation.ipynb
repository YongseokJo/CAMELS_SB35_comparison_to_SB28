{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "486adcdd",
   "metadata": {},
   "source": [
    "# Random-Split Self & Cross Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe43e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "from structures import ConventionalCNN\n",
    "from dataloader import loadCAMELS, split_expanded_dataset_from_json\n",
    "\n",
    "dtype = torch.float32\n",
    "torch.set_default_dtype(dtype)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "batch_size = 64\n",
    "individual = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916bd542",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf10cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU and CPU memory.\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def log_normalize(arr, individual=False, ref_stats=None):\n",
    "    \"\"\"Log-scale then normalize by mean/std.\"\"\"\n",
    "    arr = np.log10(arr)\n",
    "    if ref_stats:\n",
    "        mean_val, std_val = ref_stats\n",
    "    else:\n",
    "        if individual:\n",
    "            mean_val = arr.mean(axis=(1, 2), keepdims=True)\n",
    "            std_val = arr.std(axis=(1, 2), keepdims=True)\n",
    "        else:\n",
    "            mean_val, std_val = arr.mean(), arr.std()\n",
    "    return (arr - mean_val) / std_val, (mean_val, std_val)\n",
    "\n",
    "\n",
    "def _apply_sb35_windowing(data: np.ndarray, step: int = 10, length: int = 5) -> np.ndarray:\n",
    "    \"\"\"Match SB35_half/cutout sampling: take 5 frames every 10.\"\"\"\n",
    "    arr = np.arange(data.shape[0])\n",
    "    starts = np.arange(0, len(arr) - length + 1, step)\n",
    "    idx = (starts[:, None] + np.arange(length)[None, :]).reshape(-1)\n",
    "    return data[idx]\n",
    "\n",
    "\n",
    "def _apply_crop(data: np.ndarray, crop: str = \"tl\") -> np.ndarray:\n",
    "    h, w = data.shape[1], data.shape[2]\n",
    "    hh, ww = h // 2, w // 2\n",
    "    if crop == \"tl\":\n",
    "        return data[:, :hh, :ww]\n",
    "    if crop == \"tr\":\n",
    "        return data[:, :hh, ww:]\n",
    "    if crop == \"bl\":\n",
    "        return data[:, hh:, :ww]\n",
    "    if crop == \"br\":\n",
    "        return data[:, hh:, ww:]\n",
    "    raise ValueError(f\"Unknown crop: {crop}\")\n",
    "\n",
    "\n",
    "def _random_split_expanded_dataset(\n",
    "    data: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    "    chunk_size: int,\n",
    "    val_ratio: float = 0.1,\n",
    "    test_ratio: float = 0.1,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"Random train/val/test split matching train_sb random mode.\"\"\"\n",
    "    n = data.shape[0]\n",
    "    assert n % chunk_size == 0, \"data length must be divisible by chunk_size\"\n",
    "    n_chunks = n // chunk_size\n",
    "    idx = np.arange(n_chunks)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rng.shuffle(idx)\n",
    "\n",
    "    n_test = int(test_ratio * n_chunks)\n",
    "    n_val = int(val_ratio * n_chunks)\n",
    "    test_idx = idx[:n_test]\n",
    "    val_idx = idx[n_test : n_test + n_val]\n",
    "    train_idx = idx[n_test + n_val :]\n",
    "\n",
    "    def _gather(idxs):\n",
    "        flat = np.concatenate([np.arange(i * chunk_size, (i + 1) * chunk_size) for i in idxs])\n",
    "        return data[flat], labels[flat]\n",
    "\n",
    "    train = _gather(train_idx)\n",
    "    val = _gather(val_idx)\n",
    "    test = _gather(test_idx)\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "def make_random_test_loader(data_norm, labels, chunk_size, seed, val_ratio=0.1, test_ratio=0.1, shuffle_loader=False):\n",
    "    tensor = torch.tensor(data_norm, dtype=dtype)\n",
    "    labels_t = torch.tensor(labels, dtype=dtype)\n",
    "    _, _, test_set = _random_split_expanded_dataset(tensor, labels_t, chunk_size=chunk_size, val_ratio=val_ratio, test_ratio=test_ratio, seed=seed)\n",
    "    return DataLoader(test_set, batch_size=batch_size, shuffle=shuffle_loader)\n",
    "\n",
    "\n",
    "def load_model_single(path, input_shape=(256, 256)):\n",
    "    model = ConventionalCNN(input_shape=input_shape, output_shape=1, H=16, output_positive=True).to(device)\n",
    "    state = torch.load(path, map_location=device, weights_only=True)\n",
    "    model.load_state_dict(state)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_model_pair(om_path, sig_path, input_shape=(256, 256)):\n",
    "    return load_model_single(om_path, input_shape=input_shape), load_model_single(sig_path, input_shape=input_shape)\n",
    "\n",
    "\n",
    "def get_predictions(model_om, model_sig, val_loader, minmax):\n",
    "    model_om.eval()\n",
    "    model_sig.eval()\n",
    "    preds_om, preds_sig, truths = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            preds_om.append(model_om(inputs).cpu())\n",
    "            preds_sig.append(model_sig(inputs).cpu())\n",
    "            truths.append(targets.cpu())\n",
    "            del inputs\n",
    "\n",
    "    preds = np.c_[torch.cat(preds_om).numpy(), torch.cat(preds_sig).numpy()]\n",
    "    truths = torch.cat(truths).numpy()\n",
    "    del preds_om, preds_sig\n",
    "    clear_memory()\n",
    "\n",
    "    preds = preds * (minmax[:2, 1] - minmax[:2, 0]) + minmax[:2, 0]\n",
    "    mse = ((preds - truths) ** 2).mean(axis=0)\n",
    "    print(f\"  MSE: Om={mse[0]:.4e}, Sig8={mse[1]:.4e}\")\n",
    "    return truths, preds, 2\n",
    "\n",
    "\n",
    "def _metrics_point(y_true, y_pred, y_sigma=None, eps=1e-12):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    resid = y_pred - y_true\n",
    "    ss_res = np.sum(resid ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2) + eps\n",
    "    r2 = 1.0 - ss_res / ss_tot\n",
    "    rmse = np.sqrt(np.mean(resid ** 2))\n",
    "    denom = np.maximum(np.abs(y_true), eps)\n",
    "    eps_rel = np.mean(np.abs(resid) / denom)\n",
    "    if y_sigma is None:\n",
    "        chi2_red = np.nan\n",
    "    else:\n",
    "        y_sigma = np.maximum(np.asarray(y_sigma), eps)\n",
    "        chi2 = np.sum((resid / y_sigma) ** 2)\n",
    "        dof = max(int(y_true.size) - 1, 1)\n",
    "        chi2_red = chi2 / dof\n",
    "    return r2, eps_rel, rmse, chi2_red\n",
    "\n",
    "\n",
    "def _metrics_with_uncertainty(y_true, y_pred, y_sigma=None, n_boot=200, seed=0):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    if y_sigma is not None:\n",
    "        y_sigma = np.asarray(y_sigma)\n",
    "    n = y_true.shape[0]\n",
    "    if n <= 1:\n",
    "        r2, eps_rel, rmse, chi2_red = _metrics_point(y_true, y_pred, y_sigma=y_sigma)\n",
    "        return (r2, 0.0), (eps_rel, 0.0), (rmse, 0.0), (chi2_red, 0.0)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    samples = np.empty((n_boot, 4), dtype=float)\n",
    "    for b in range(n_boot):\n",
    "        idx = rng.integers(0, n, size=n)\n",
    "        yt = y_true[idx]\n",
    "        yp = y_pred[idx]\n",
    "        ys = y_sigma[idx] if y_sigma is not None else None\n",
    "        samples[b] = _metrics_point(yt, yp, y_sigma=ys)\n",
    "    means = samples.mean(axis=0)\n",
    "    stds = samples.std(axis=0, ddof=1)\n",
    "    return (means[0], stds[0]), (means[1], stds[1]), (means[2], stds[2]), (means[3], stds[3])\n",
    "\n",
    "\n",
    "def plot_comparison(data, chunk_size, title, num_samples=200, n_boot=200, seed=0):\n",
    "    truths, preds, _ = data\n",
    "    target_names = [r\"$\\Omega_m$\", r\"$\\sigma_8$\"]\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    chunks = preds.reshape(-1, chunk_size, 2)\n",
    "    pred_mean = chunks.mean(axis=1)\n",
    "    pred_std = chunks.std(axis=1)\n",
    "    true = truths[::chunk_size]\n",
    "    n = min(num_samples, true.shape[0])\n",
    "    for i, ax in enumerate(axs):\n",
    "        x = true[:n, i]\n",
    "        y = pred_mean[:n, i]\n",
    "        yerr = pred_std[:n, i]\n",
    "        (r2_m, r2_s), (eps_m, eps_s), (rmse_m, rmse_s), (chi2_m, chi2_s) = _metrics_with_uncertainty(x, y, y_sigma=yerr, n_boot=n_boot, seed=seed + i)\n",
    "        eps_pct_m = 100.0 * eps_m\n",
    "        eps_pct_s = 100.0 * eps_s\n",
    "        lines = [\n",
    "            rf\"$R^2 = {r2_m:.4f} \\pm {r2_s:.4f}$\",\n",
    "            rf\"$\\epsilon = ({eps_pct_m:.2f} \\pm {eps_pct_s:.2f})\\%$\",\n",
    "            rf\"$\\mathrm{RMSE} = {rmse_m:.3e} \\pm {rmse_s:.3e}$\",\n",
    "            rf\"$\\chi^2_\\nu = {chi2_m:.3e} \\pm {chi2_s:.3e}$\",\n",
    "        ]\n",
    "        textstr = \"\\n\".join(lines)\n",
    "        ax.errorbar(x, y, yerr=yerr, fmt='none', capsize=2, ecolor='tab:orange', alpha=0.7)\n",
    "        ax.scatter(x, y, s=4, c='k', zorder=10)\n",
    "        ax.plot([x.min(), x.max()], [x.min(), x.max()], 'r--', label='Ideal')\n",
    "        ax.text(0.95, 0.05, textstr, transform=ax.transAxes, fontsize=9, va='bottom', ha='right', bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.5))\n",
    "        ax.set_xlabel(\"Truth\")\n",
    "        ax.set_ylabel(\"Prediction\")\n",
    "        ax.set_title(target_names[i])\n",
    "        ax.legend(loc='upper left')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    fig.suptitle(title, fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def compute_stats(data):\n",
    "    truths, preds, _ = data\n",
    "    return {\n",
    "        'bias_om': np.mean(truths[:, 0] - preds[:, 0]),\n",
    "        'scatter_om': np.std(truths[:, 0] - preds[:, 0]),\n",
    "        'bias_sig': np.mean(truths[:, 1] - preds[:, 1]),\n",
    "        'scatter_sig': np.std(truths[:, 1] - preds[:, 1]),\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_scorebox_scores(data, chunk_size, n_boot=200, seed=0):\n",
    "    truths, preds, _ = data\n",
    "    chunks = preds.reshape(-1, chunk_size, 2)\n",
    "    pred_mean = chunks.mean(axis=1)\n",
    "    pred_std = chunks.std(axis=1)\n",
    "    true = truths[::chunk_size]\n",
    "    out = {}\n",
    "    for i, tag in enumerate(['om', 'sig']):\n",
    "        x = true[:, i]\n",
    "        y = pred_mean[:, i]\n",
    "        yerr = pred_std[:, i]\n",
    "        (r2_m, r2_s), (eps_m, eps_s), (rmse_m, rmse_s), (chi2_m, chi2_s) = _metrics_with_uncertainty(x, y, y_sigma=yerr, n_boot=n_boot, seed=seed + i)\n",
    "        out[f'r2_{tag}_m'] = float(r2_m)\n",
    "        out[f'r2_{tag}_s'] = float(r2_s)\n",
    "        out[f'eps_{tag}_pct_m'] = float(100.0 * eps_m)\n",
    "        out[f'eps_{tag}_pct_s'] = float(100.0 * eps_s)\n",
    "        out[f'rmse_{tag}_m'] = float(rmse_m)\n",
    "        out[f'rmse_{tag}_s'] = float(rmse_s)\n",
    "        out[f'chi2_{tag}_m'] = float(chi2_m)\n",
    "        out[f'chi2_{tag}_s'] = float(chi2_s)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _ensure_dir(path: Path) -> None:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def _slugify(name: str) -> str:\n",
    "    return name.replace(':', '').replace(' ', '_').replace('|', '-').replace('>', 'to').replace('/', '-')\n",
    "\n",
    "\n",
    "def run_evaluation(model_om, model_sig, val_loader, minmax, chunk_size, title, num_samples=200, save_dir=None, base_name=None):\n",
    "    print(f\"\\n{title}\")\n",
    "    data = get_predictions(model_om, model_sig, val_loader, minmax)\n",
    "    fig = plot_comparison(data, chunk_size, title, num_samples=num_samples)\n",
    "    if save_dir is not None:\n",
    "        _ensure_dir(Path(save_dir))\n",
    "        stem = _slugify(base_name or title)\n",
    "        fig.savefig(Path(save_dir) / f\"{stem}.png\", dpi=200, bbox_inches='tight')\n",
    "        fig.savefig(Path(save_dir) / f\"{stem}.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    stats = compute_stats(data)\n",
    "    stats.update(compute_scorebox_scores(data, chunk_size=chunk_size, n_boot=200, seed=0))\n",
    "    del data, fig\n",
    "    clear_memory()\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563345b7",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52713d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = Path(\"../data/models\")\n",
    "chunk_sb28 = 15\n",
    "chunk_sb35_cutout = 15\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "seeds = [0, 42, 123, 456, 789]\n",
    "\n",
    "results_dir = Path(\"../plot/random_cross_eval\")\n",
    "plots_dir = results_dir / \"figures\"\n",
    "tables_dir = results_dir / \"tables\"\n",
    "_ensure_dir(plots_dir)\n",
    "_ensure_dir(tables_dir)\n",
    "\n",
    "all_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3b555b",
   "metadata": {},
   "source": [
    "## Load Data and Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347f6ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SB28\n",
    "_dataSB28, _labelsSB28, minmaxSB28 = loadCAMELS(field=\"Mtot\", box=\"SB28\", normalization=False, individual=individual)\n",
    "labelsSB28 = torch.tensor(_labelsSB28[:, :2], dtype=dtype)\n",
    "dataSB28_norm, statsSB28 = log_normalize(_dataSB28, individual=individual)\n",
    "\n",
    "# SB35 cutout\n",
    "_dataSB35, _labelsSB35, minmaxSB35 = loadCAMELS(field=\"Mtot\", box=\"SB35\", normalization=False, individual=individual)\n",
    "labelsSB35 = torch.tensor(_labelsSB35[:, :2], dtype=dtype)\n",
    "dataSB35_window = _apply_sb35_windowing(_dataSB35)\n",
    "dataSB35_cutout = _apply_crop(dataSB35_window, crop=\"tl\")\n",
    "dataSB35_cutout_norm, statsSB35_full = log_normalize(dataSB35_cutout, individual=individual)\n",
    "statsSB35_cutout = statsSB35_full\n",
    "\n",
    "del dataSB35_window\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadc8345",
   "metadata": {},
   "source": [
    "## Self Validation (random splits by seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facfbef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_results = []\n",
    "\n",
    "for seed in seeds:\n",
    "    # SB28 self\n",
    "    loader_sb28 = make_random_test_loader(dataSB28_norm, labelsSB28, chunk_size=chunk_sb28, seed=seed, val_ratio=val_ratio, test_ratio=test_ratio)\n",
    "    model_om, model_sig = load_model_pair(model_dir / f\"SB28_om_seed{seed}_best.pt\", model_dir / f\"SB28_sig_seed{seed}_best.pt\", input_shape=(256, 256))\n",
    "    res = run_evaluation(model_om, model_sig, loader_sb28, minmaxSB28, chunk_sb28, f\"SB28 self (seed={seed})\", save_dir=plots_dir, base_name=f\"sb28_seed{seed}_self\")\n",
    "    self_results.append((f\"SB28 self seed {seed}\", res))\n",
    "    del loader_sb28, model_om, model_sig\n",
    "    clear_memory()\n",
    "\n",
    "    # SB35_cutout self\n",
    "    loader_sb35 = make_random_test_loader(dataSB35_cutout_norm, labelsSB35, chunk_size=chunk_sb35_cutout, seed=seed, val_ratio=val_ratio, test_ratio=test_ratio)\n",
    "    model_om, model_sig = load_model_pair(model_dir / f\"SB35_cutout15_om_seed{seed}_best.pt\", model_dir / f\"SB35_cutout15_sig_seed{seed}_best.pt\", input_shape=(256, 256))\n",
    "    res = run_evaluation(model_om, model_sig, loader_sb35, minmaxSB35, chunk_sb35_cutout, f\"SB35_cutout self (seed={seed})\", save_dir=plots_dir, base_name=f\"sb35cutout_seed{seed}_self\")\n",
    "    self_results.append((f\"SB35_cutout self seed {seed}\", res))\n",
    "    del loader_sb35, model_om, model_sig\n",
    "    clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876989e3",
   "metadata": {},
   "source": [
    "## Cross Validation SB28 ↔ SB35_cutout (matching seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f8802",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_results = []\n",
    "\n",
    "for seed in seeds:\n",
    "    # Prepare SB35_cutout data normalized with SB28 stats for SB28 model\n",
    "    data_cutout_norm_sb28, _ = log_normalize(dataSB35_cutout, individual=individual, ref_stats=statsSB28)\n",
    "    loader_sb35_for_sb28 = make_random_test_loader(data_cutout_norm_sb28, labelsSB35, chunk_size=chunk_sb35_cutout, seed=seed, val_ratio=val_ratio, test_ratio=test_ratio)\n",
    "    model_om, model_sig = load_model_pair(model_dir / f\"SB28_om_seed{seed}_best.pt\", model_dir / f\"SB28_sig_seed{seed}_best.pt\", input_shape=(256, 256))\n",
    "    res = run_evaluation(model_om, model_sig, loader_sb35_for_sb28, minmaxSB28, chunk_sb35_cutout, f\"SB28 model on SB35_cutout (seed={seed})\", save_dir=plots_dir, base_name=f\"sb28_seed{seed}_to_sb35cutout\")\n",
    "    cross_results.append((f\"SB28 → SB35_cutout seed {seed}\", res))\n",
    "    del data_cutout_norm_sb28, loader_sb35_for_sb28, model_om, model_sig\n",
    "    clear_memory()\n",
    "\n",
    "    # Prepare SB28 data normalized with SB35_cutout stats for SB35 model\n",
    "    data_sb28_norm_sb35, _ = log_normalize(_dataSB28, individual=individual, ref_stats=statsSB35_cutout)\n",
    "    loader_sb28_for_sb35 = make_random_test_loader(data_sb28_norm_sb35, labelsSB28, chunk_size=chunk_sb28, seed=seed, val_ratio=val_ratio, test_ratio=test_ratio)\n",
    "    model_om, model_sig = load_model_pair(model_dir / f\"SB35_cutout15_om_seed{seed}_best.pt\", model_dir / f\"SB35_cutout15_sig_seed{seed}_best.pt\", input_shape=(256, 256))\n",
    "    res = run_evaluation(model_om, model_sig, loader_sb28_for_sb35, minmaxSB35, chunk_sb28, f\"SB35_cutout model on SB28 (seed={seed})\", save_dir=plots_dir, base_name=f\"sb35cutout_seed{seed}_to_sb28\")\n",
    "    cross_results.append((f\"SB35_cutout → SB28 seed {seed}\", res))\n",
    "    del data_sb28_norm_sb35, loader_sb28_for_sb35, model_om, model_sig\n",
    "    clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f09cd1",
   "metadata": {},
   "source": [
    "## Save Summary Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f3883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pm(m, s, fmt):\n",
    "    if m is None or s is None:\n",
    "        return 'nan'\n",
    "    if not np.isfinite(m) or not np.isfinite(s):\n",
    "        return 'nan'\n",
    "    return (fmt.format(m) + ' ± ' + fmt.format(s))\n",
    "\n",
    "def _write_table(results, prefix):\n",
    "    lines = []\n",
    "    lines.append('=' * 110)\n",
    "    lines.append(f'{prefix} SUMMARY')\n",
    "    lines.append('=' * 110)\n",
    "    header = f\"{'Experiment':<40} {'Target':<6} {'R^2':>18} {'epsilon[%]':>18} {'RMSE':>18} {'chi2_nu':>18}\"\n",
    "    lines.append(header)\n",
    "    lines.append('-' * 110)\n",
    "    print('\n",
    "' + lines[0])\n",
    "    print(lines[1])\n",
    "    print(lines[0])\n",
    "    print(header)\n",
    "    print('-' * 110)\n",
    "    for name, stats in results:\n",
    "        for tag, label in [('om', 'Om'), ('sig', 'Sig8')]:\n",
    "            r2 = _pm(stats.get(f'r2_{tag}_m'), stats.get(f'r2_{tag}_s'), '{:.4f}')\n",
    "            eps = _pm(stats.get(f'eps_{tag}_pct_m'), stats.get(f'eps_{tag}_pct_s'), '{:.2f}') + '%'\n",
    "            rmse = _pm(stats.get(f'rmse_{tag}_m'), stats.get(f'rmse_{tag}_s'), '{:.3e}')\n",
    "            chi2 = _pm(stats.get(f'chi2_{tag}_m'), stats.get(f'chi2_{tag}_s'), '{:.3e}')\n",
    "            row = f\"{name:<40} {label:<6} {r2:>18} {eps:>18} {rmse:>18} {chi2:>18}\"\n",
    "            print(row)\n",
    "            lines.append(row)\n",
    "        print('-' * 110)\n",
    "        lines.append('-' * 110)\n",
    "    with open(tables_dir / f\"{prefix.lower()}_summary.txt\", 'w') as f:\n",
    "        f.write('\n",
    "'.join(lines))\n",
    "    tsv = ['Experiment\tTarget\tR2\tR2_std\tepsilon_pct\tepsilon_pct_std\tRMSE\tRMSE_std\tchi2_nu\tchi2_nu_std']\n",
    "    for name, stats in results:\n",
    "        for tag, label in [('om', 'Om'), ('sig', 'Sig8')]:\n",
    "            r2m, r2s = stats.get(f'r2_{tag}_m'), stats.get(f'r2_{tag}_s')\n",
    "            epsm, epss = stats.get(f'eps_{tag}_pct_m'), stats.get(f'eps_{tag}_pct_s')\n",
    "            rmsem, rmses = stats.get(f'rmse_{tag}_m'), stats.get(f'rmse_{tag}_s')\n",
    "            chi2m, chi2s = stats.get(f'chi2_{tag}_m'), stats.get(f'chi2_{tag}_s')\n",
    "            tsv.append(f\"{name}\t{label}\t{r2m}\t{r2s}\t{epsm}\t{epss}\t{rmsem}\t{rmses}\t{chi2m}\t{chi2s}\")\n",
    "    with open(tables_dir / f\"{prefix.lower()}_summary.tsv\", 'w') as f:\n",
    "        f.write('\n",
    "'.join(tsv))\n",
    "\n",
    "_write_table(self_results, 'SELF')\n",
    "_write_table(cross_results, 'CROSS')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
